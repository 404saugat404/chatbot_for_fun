{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anJupEh4tpRu",
        "outputId": "37158fe3-4458-438b-c30b-644ffe6ae455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in .\\env\\lib\\site-packages (4.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in .\\env\\lib\\site-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in .\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\env\\lib\\site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in .\\env\\lib\\site-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in .\\env\\lib\\site-packages (from langchain) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in .\\env\\lib\\site-packages (from langchain) (0.1.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in .\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in .\\env\\lib\\site-packages (from langchain) (2.7.2)\n",
            "Requirement already satisfied: requests<3,>=2 in .\\env\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in .\\env\\lib\\site-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in .\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in .\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in .\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone-client in .\\env\\lib\\site-packages (3.2.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in .\\env\\lib\\site-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in .\\env\\lib\\site-packages (from pinecone-client) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in .\\env\\lib\\site-packages (from pinecone-client) (4.12.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in .\\env\\lib\\site-packages (from pinecone-client) (2.2.1)\n",
            "Requirement already satisfied: colorama in .\\env\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcTuUx3Zyi3g",
        "outputId": "1fc98b1e-d041-44cc-c368-5623e89ce8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in .\\env\\lib\\site-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in .\\env\\lib\\site-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\env\\lib\\site-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\env\\lib\\site-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in .\\env\\lib\\site-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in .\\env\\lib\\site-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in .\\env\\lib\\site-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in .\\env\\lib\\site-packages (from langchain-community) (0.1.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in .\\env\\lib\\site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in .\\env\\lib\\site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in .\\env\\lib\\site-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in .\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in .\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in .\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in .\\env\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in .\\env\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in .\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\env\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in .\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in .\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in .\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in .\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Ne6dgPWhy_5v"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import retrieval_qa\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "2_Sn2qhVhflC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ICFG8QyouM2s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "google_api_key = os.getenv('google_api_key')\n",
        "\n",
        "if google_api_key is None:\n",
        "    raise ValueError(\"API key not found. Please ensure it is set in the .env file.\")\n",
        "\n",
        "# Configure the generative AI with the API key\n",
        "genai.configure(api_key=google_api_key)\n",
        "\n",
        "# Initialize the GenerativeModel\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Now you can use the model for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Bd6ReBpxuQ-A",
        "outputId": "220a691b-05e3-43e6-9f2e-5c371235e828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 + 2 = 4 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\"whats 2+2\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "kPsmXiJemLzF",
        "outputId": "6f482b6b-de41-45b3-b8b3-5249c39fdbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n",
            "Hello! How can I help you today? \n",
            "\n",
            "what is the fullform of os\n",
            "OS stands for **Operating System**. \n",
            "\n",
            "what is your name\n",
            "I am a large language model, so I don't have a name in the same way that a person does. You can call me Bard, or anything else you like! \n",
            "\n",
            "ok\n",
            "Okay! \n",
            "\n",
            "What can I do for you?  Tell me what's on your mind. I'm ready to help. \n",
            "\n",
            "exit\n",
            "Okay, I'm exiting now.  Is there anything else I can help you with before I go? \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def response(text):\n",
        "  response1 = model.generate_content(text)\n",
        "  return response1.text\n",
        "\n",
        "text=input(\"write yes if you want to get started: \")\n",
        "if text=='yes':\n",
        "  while(text!=\"exit\"):\n",
        "    text=input(\"enter your text: \")\n",
        "    print(text)\n",
        "    print(response(text))\n",
        "\n",
        "else:\n",
        "  exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "hVf1fVjjovVA"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"D:\\cb\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf\")\n",
        "pdf_pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM5LylLdZjYE",
        "outputId": "98ca146f-263e-4232-f950-0dc55c20eba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=' Research of  YOLO Architecture Models in  Book \\nDetection  \\nMaria Kalinina  \\nDepartment 316  \\nMoscow Aviation Institute \\n(National Research \\nUniversity)  \\nMoscow, Russia  \\nPtaha -96@yandex.ru  \\n Pavel Nikolaev * \\nDepartment 316  \\nMoscow Aviation Institute \\n(National Research \\nUniversity)  \\nMoscow, Russia  \\nnpavel89@gmail.com    \\nAbstract— Deep neural networks are widely used in different \\nfields of human activity, including spheres which are  connected \\nwith large amount of  operations such as data  obtaining and \\nprocessing information from the outside world. This article \\ndeals with the creation of the deep  convolutional neural network \\nbased on the YOLO architecture for book detection in real time. \\nThe architecture chosen as the basis of the neural n etwork \\npossesses a number of advantages which  make it highly \\ncompetitive with other models, so it can be considered as  the \\nmost suitable option  for the creation of deep neural network for \\nobject detection.  Creation of the original dataset and the deep \\nneur al network training are described. Several variants of \\nneural networks based on the YOLO architecture are discussed  \\nand the results of their comparison are shown . The results \\nobtained during the training of a deep neural network allow us \\nto use it as a bas is for further development of the application.  \\nKeywords —image recognition , object detection , computer \\nvision , machine learning , artificial neural networks , deep \\nlearning , convolutional neural networks  \\nI. INTRODUCTION  \\nAt the present time  neural networks are wi dely used in \\nvarious spheres of human society and often act as an assistant \\nin solution of many important problems which is based on the \\nobject detection in many cases. Among various types of \\nnetworks convolutional neural networks  (CNN) show the best \\nresul ts in image recognition.  \\nThe use of neural networks can greatly facilitate activities \\nin various areas, for example, those that deal with large \\namounts of data, for instance , while working with a large \\nnumber of books, which is typical for bookstores, libraries or \\nwarehouses. Search of the definite books based on the use of \\na neural network capable to localize book spines can be more \\nefficient and faster than in case if it is  performed manually. On \\nthe basis of such a neural network, it is possible to create a \\nspecialized application that can act, for example, as an \\ninventory search engine or an independent mobile application.  \\nThis article discusses the development of a neural  network \\nfor detecting books on bookshelves with the use of book \\nspines. Book detection is supposed to be performed in real \\ntime. In this regard, it is necessary to build and train a neural \\nnetwork that can recognize a sufficient number of frames per \\nsecon d and at the same time give fairly accurate results.  \\nThe neural network should analyze the input image for the \\npresence of books on it and provide the user with information about the detected objects. In other words, at the output the \\nlocation of the books  should be marked on the submitted \\nimage with the use of  a bounding box (\"border\" around the \\nspine of the detected book) located on the book spine.  \\nThe main tasks of this work are to create and compare \\nseveral models of the CNN for detecting books and thei r \\nfurther training and comparison of the results obtained in order \\nto identify the optimal model.  \\nII. YOLO  CONVOLUTIONAL NETWO RK \\nNowadays deep convolutional networks successfully act \\nas the systems of deep learning and show good results in \\ndifferent tasks, for  instance, in case of image classification, \\nobject detection and segmentation.  \\nThe typical architecture of CNN is based on the mixture of \\nconvolutional and pooling layers. While passing through it \\npicture transforms into the feature map which consequently \\ngoes to fully -connected layers.  \\nYou Only Look Once ( YOLO ) architecture refers to the \\ntype of one -shot detectors  [1-3]. This type of networks is \\nnotable for remarkable speed of work, however precision of \\nobtained results due to the single passing of the ima ge through \\nthe network can be a little lower. It can be quite useful to put \\nin practice  this type of network as a basement for software if \\nit deals with the real time functioning and allows to recognize \\nsufficient quantity of frames.  \\nAccording to the resu lts of researches [3] the last \\nmodification of YOLO – YOLOv3 is turning out to \\ndemonstrate the highest speed of work and precision in \\ncomparison with other networks of the same type of detectors. \\nYOLO can be used for solving problems of different type s, for \\nexample, for ship detection [ 4], vehicle traffic analysis [ 5], \\ntraffic signposts [ 6], bridge damage detection [ 7], in case of \\nmedical tasks [ 8], etc.  \\nOne of the important features of YOLO -based networks is \\nthe use of grid of a definite size which superim poses the input \\nimage and divide s it into a number of cells. Each cell is given \\nan array of predicted values: x and y – the coordinates of the \\nbounding box (upper left and lower right points) or the \\ncoordinates of the center of the bounding box, w and h – width \\nand height of the bounding box and also С – the probability of \\nthe class to which object detected in the image belong. The \\ninput image of the neural network is divided into S cells. For \\nAdvances in Intelligent Systems Research, volume 174\\nProceedings of the 8th Scientific Conference on Information Technologies for\\nIntelligent Decision Making Support (ITIDS 2020)\\nCopyright © 2020 The Authors. Published by Atlantis Press B.V.\\nThis is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 218', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content=\"each object in the picture there is a cell which is responsible \\nfor its prediction (the cell where the center of the object falls \\nin). For each cell prediction is performed for B bounding \\nboxes and C probabilistic estimates of classes. Thus, the \\nterminal output takes the shape of S x S x (B х 5 +  C)-\\ndimensional tensor.  \\nAfter receiving the input image the network performs \\nnecessary operations to split it into cells. During this process \\nthe non -max suppression algorithm is applied with the \\npurpose of elimination of unnecessary predictions.  \\nThe input data for the algorithm  include a list with the \\nfollowing elements: values of the specified bounding boxes, \\nthe probability of the presence of the desired object on the \\nimage and the overlap thresholds. As a result the output data \\npresent a list of predictions which have success fully passed \\nsome kind of filtering which can be described as a set of \\ndefinite actions: the preliminary created empty list is filled \\nwith the prediction with the maximum value of the probability \\nof the object's presence in the image, then Intersection Ove r \\nUnion ( IOU) values for this and all other predictions are \\ncalculated. If IOU values exceed the value of specified overlap \\nthreshold the predicted bounding box it belongs to is deleted. \\nIn the end, the frame coordinates (or center coordinates with \\nwidth a nd height) are obtained in the last convolutional layer.  \\nOur network is intended to be used for detection of objects \\nof a single class. It means that the output data of our network \\nwill be presented in the form of a massive which contains 4 \\nvalues.  \\nAccordi ng to [ 1], the YOLO network has a number of \\nadvantages such as very high speed of work, which allows it \\nto recognize up to 35 frames per second (according to the \\nspeed of the latest version of YOLO on the COCO data set \\nwas 35 fps  [9]), possibility to opera te with the whole image at \\nonce, not with its individual parts and to study generalized \\nrepresentations of objects. All these peculiarities cause better \\nwork of the network on new data and fewer mistakes while \\nseparating the desired objects from the backgr ound.  \\nBatch -normalization, a method mostly oriented on \\nnormalizing input layers by scaling the dataset, can be applied \\nin order to increase the stabilization and performance of neural \\nnetworks. Batch -normalization is based on preliminary \\nprocessing of data  that the network will later operate on, to a \\nstate where the set of values submitted to the input of the \\nneural network which is characterized by a zero mathematical \\ndistribution and the variance is 0.  \\nLeaky ReLU which a modified version of the normal \\nReLU function is often used as an activation function in \\nYOLO networks. This modification allows successfully use \\nthe activation function in cases where conventional ReLU \\nfails, for example, if the neural network training speed is too \\nhigh.  \\nIII. NETWORK FOR BOOK D ETECTION  \\nYOLO -based network for book detection was trained from \\nscratch, without using ready -made scales. A specially created \\ndataset based on 500 images was used in process of network \\ntraining.  \\nEach image corresponds to one of the markup files in xml \\nform at with the total information for the objects of the desired class: class labels, coordinates of the bounding box for each \\nobject, name of the file, height and width of the image. The \\ncoordinates of the bounding box are xmin and ymin for the \\nupper -left cor ners and xmax and ymax for lower -right ones. \\nImage markup was performed in a form similar to the markup \\nform of the PascalVOC dataset.  \\nIn our case the used data set is not large enough and it is \\nsufficiently homogeneous so we decide to apply the \\naugmentati on technique to expand the initial data volume \\ncontained in the dataset we have already created. This means \\nthat new elements were artificially generated on the base of \\nthe original dataset. Augmentation was performed on \\nrandomly selected images from the d ataset and included the \\nactions such as scaling, rotation, flip, cropping distortion of \\ncolours and so on.  \\nTo assess the quality of neural networks various metrics \\nare used. Among them there is one of the most popular metrics \\nknown as average precision (AP ). It is used in various tasks, \\nfor example, in cases when we need to evaluate the quality of \\nranking, classification, or objects detection. The AP for a \\nseries of queries can be defined as the average value of the \\naverage accuracy estimates for each query  [10]. In our case, \\nthe metric determines the percentage of correctly detected \\nobjects. Mathematically, AP is a calculation of the values of \\np(r) – the recall function -in the interval of values r = (0; 1).  \\nFor the programming language basement for our netw orks \\nwe use Python 3.6. Creation of CNN structure for book \\ndetection was done with the use of Python libraries for deep \\nlearning – Keras 2.2.4 and TensorFlow 1.12.0.  \\nThe dataset on which the deep neural network was trained \\nconsist s of 500 images with 5,245  book spines  depicted on \\nthem. Further, the dataset was divided in certain percentage : \\ninto training (60% of pictures), validation (10%) and test \\n(30%) sets. Thus, 300 pictures with frames were included in \\nthe test set, 50 in the validation set, and 150 in  the test set. \\nMoreover, the number of frames in the markup for each \\npicture was individual.  \\nIn the process of creating a neural network, several \\nvariants of the network structure were tested, including one \\nclose to Tiny YOLOv3 . Tiny YOLOv3  is a modificati on of \\nYOLO architecture which is characterized by 2 outputs in \\ncomparison with standard 3 outputs in full YOLO networks \\nversion. It also has a simplified layer structure which ma kes \\nTiny YOLOv3  networks more adaptable for functioning on a \\nmobile platform.  \\nFor Y OLO v3 architecture networks there are 3 outputs, \\neach is responsible for recognizing objects of a certain size; so \\none of the outputs with the highest resolution is responsible \\nfor detecting small objects, the output with the lowest \\nresolution detect s objects that are larger. Accordingly, an \\noutput with an average resolution value recognizes medium -\\nsized objects.  \\nIV. RESULTS  \\nIn addition to the standard Y OLO v3 with 3 outputs, we \\nalso tested the modified Y OLO v3 network with 2 outputs, \\nwhich are typical for Tiny YOLOv3 , however, in this case \\nAdvances in Intelligent Systems Research, volume 174\\n219\", metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='there is high probability of overfitting, while the standard \\nnetwork can be trained further in order to improve the results.  \\nAll of the deep neural networks models were  trained \\nduring 800 epochs . We trained our network models for book \\ndetection on the GPU Nvidia GeForce GTX 1060 6GB  with \\nfollowing set of network parameters:  \\n  \\nFig. 1. Changes in error values in training and validation datasets for \\nYOLOv3 Mo del \\n \\n Fig. 2. Changes in error values in training and validation datasets for \\nYOLOv3 Model with 2 outputs  \\n \\n Fig. 3. Changes in error values in training and validation datasets for \\nTiny-YOLOv3 Model  \\n \\nFig. 4. Example of the neural network work in book detection for \\nYOLOv3 Model  \\n \\nFig. 5. Example of the neural network work in book detection for \\nYOLOv3 Model with 2 outputs  \\n \\nFig. 6. Example of the neural network work in book detection for Tiny -\\nYOLOv3 Model  \\nAdvances in Intelligent Systems Research, volume 174\\n220', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 2}),\n",
              " Document(page_content=\"1. size of input image – 448x448x3; \\n2. optimizer – Adam;  \\n3. the learning rate – 0,0001;  \\n4. batch -size – 2 samples;  \\n5. metric for assessing the correctness of the class  \\ndefinition – AP. \\nDuring training of the deep neural networks the error \\nfunction was minimized. The error function can be defined as \\nthe sum of errors from each output, so in the models with two \\noutputs the error is less than in the models with three once. \\nThus, it is neces sary to evaluate the network efficiency by the \\nAP value.  \\nThe training graphs with the obtained error values for 3 \\ndifferent network models – standard YOLOv3  with 3 outputs , \\nYOLOv3 with 2 outputs and Tiny -YOLOv3 are shown in Fig. \\n1, 2, 3.  \\nTable 1 shows the best values of AP and loss values for 3 \\ndatasets – training, validation and testing obtained during \\nnetwork training for each of 3 tested network models.  \\nTABLE I.  THE  RESULTS  OF YOLO -BASED  NETWORKS  \\nCOMPARISON  \\nYOLO \\nmodel  Training set  Validation set  Testing set  \\nAP Loss AP Loss AP Loss \\nYOLOv3 \\n(3 outputs)  93.74%  7.38 91.76%  6.57 94.43%  6.74 \\nYOLOv3 \\n(2 outputs)  89.29 % 4.53 91.07 % 3.72 92.35 % 4.42 \\nTiny \\nYOLOv3  84.98 % 5.96 88.42 % 6.23 88.75 % 6.98 \\n \\nFig. 4, 5 and 6 show the examples of the functioning of the \\nneural networks after training. Above each frame (bounding \\nbox with required object) there are the probability values \\nwhich corresponds to the probability of found object to belong \\nto a given class. We see that the neural network s quite \\nconfidently learne d to recognize and detect books on book \\nroots.  However, the best results were achieved when using the \\noriginal YOLOv3 architecture structure.  \\nIn our case, experiments with reducing of the number of \\nlayers did not bring much success, while increasing their \\nnumber on the contrary led to improved results. However, if we increase the number of layers, the network may become \\nheavier and the speed of its operation may decrease. We plan \\nto discuss this issue in the next papers.  \\nV. CONCLUSIONS  \\nThis article is dedicate d to the creation of a neural network \\nfor book detection on book spines . As a basis for it several \\nmodels of YOLOv3 architecture which is distinguished by a \\nsignificant speed of obtaining output data and a relatively \\nsmall error value were used. In the process of training neural \\nnetworks a comparison of the quality of their work was made \\nand the best network was selected. The results obtained after \\ntraining a deep neural network showed that the network has \\nlearned how to recognize and detect objects of a  given class \\nconfidently. In the future we are planning to continue our work \\non its improvement.  \\nREFERENCES  \\n[1] J. Redmon, S. Divvala, R. Girshick, A. Farhadi. “You only look once: \\nUnified, real -time object detection,” Proc. 2016 IEEE Conference on \\nComputer Vi sion and Pattern Recognition, pp. 779 –788, 2016.  \\n[2] J. Redmon, A. Farhady. “Yolo9000: Better, faster, stronger,” Proc. \\n2017 IEEE Conference on Computer Vision and Pattern Recognition, \\npp. 6517 –6525, 2017.  \\n[3] J. Redmon, A. Farhady. “YOLOv3: An Incremental Improvement,” \\nProc. 2018 IEEE Conference on Computer Vision and Pattern \\nRecognition, 2018.  \\n[4] Y. Chang , A. Anagaw, L. Chang, Y. Chun Wang, C. Hsiao, W. Lee. \\n“Ship Detection Based on YOLOv2 for SAR Imagery,” Rem ote \\nSensing – Open Access Journal, 2 April 2019.  \\n[5] C. Rajesh Babu, G. Anirudh. “Vehicle Traffic Analysis Using Yolo,” \\nEurasian Journal of Analytical Chemistry, vol. 13, pp. 345 –350, 2018.  \\n[6] A. V. Devyatkin, D. M. Filatov. “Neural network traffic signs detectio n \\nsystem development,”  Proc. International conference on soft \\ncomputing and measurement, vol. 1, pp. 189 -192, 2019 [Nejrosetevaya \\nsistema obnaruzheniya znakov dorozhnogo dvizheniya].  \\n[7] C. Zhang, C.C. Chang, M. Jamshidi. “Bridge Damage Detection using \\na Sing le-Stage Detector and Field Inspection Images,” Computer \\nSociety, 8 April 2018.  \\n[8] K.S. Kurochka, T.V. Luchsheva, K.A. Panarin. “Localization of human \\npercentages on X -ray images with use of Darknet YOLO,” Doklady \\nBGUIR, Vol. 113, Nо. 3, pp. 32 –38, 2018 [Loka lizaciya pozvonkov \\ncheloveka na rentgenovskih izobrazheniyah s ispol'zovaniem \\nDARKNET YOLO].  \\n[9]  “YOLO: Real -Time Object Detection ”.  Source: \\nhttps://pjreddie.com/darknet/yolo/  \\n[10] T.C. Arlen. “Understanding the mAP Evaluation Metric for Object \\nDetection,” Medium, 1 March 2018.  \\n \\n \\n  \\n \\nAdvances in Intelligent Systems Research, volume 174\\n221\", metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3})]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "osko-nS2uQVk"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "2UyzDblDu93l"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(pdf_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qmVoSaKuTmV",
        "outputId": "b068d568-5008-4a74-b2f0-191910247102"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Research of  YOLO Architecture Models in  Book \\nDetection  \\nMaria Kalinina  \\nDepartment 316  \\nMoscow Aviation Institute \\n(National Research \\nUniversity)  \\nMoscow, Russia  \\nPtaha -96@yandex.ru  \\n Pavel Nikolaev * \\nDepartment 316  \\nMoscow Aviation Institute \\n(National Research \\nUniversity)  \\nMoscow, Russia  \\nnpavel89@gmail.com    \\nAbstract— Deep neural networks are widely used in different \\nfields of human activity, including spheres which are  connected', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='with large amount of  operations such as data  obtaining and \\nprocessing information from the outside world. This article \\ndeals with the creation of the deep  convolutional neural network \\nbased on the YOLO architecture for book detection in real time. \\nThe architecture chosen as the basis of the neural n etwork \\npossesses a number of advantages which  make it highly \\ncompetitive with other models, so it can be considered as  the', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='most suitable option  for the creation of deep neural network for \\nobject detection.  Creation of the original dataset and the deep \\nneur al network training are described. Several variants of \\nneural networks based on the YOLO architecture are discussed  \\nand the results of their comparison are shown . The results \\nobtained during the training of a deep neural network allow us \\nto use it as a bas is for further development of the application.', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='Keywords —image recognition , object detection , computer \\nvision , machine learning , artificial neural networks , deep \\nlearning , convolutional neural networks  \\nI. INTRODUCTION  \\nAt the present time  neural networks are wi dely used in \\nvarious spheres of human society and often act as an assistant \\nin solution of many important problems which is based on the \\nobject detection in many cases. Among various types of \\nnetworks convolutional neural networks  (CNN) show the best', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='resul ts in image recognition.  \\nThe use of neural networks can greatly facilitate activities \\nin various areas, for example, those that deal with large \\namounts of data, for instance , while working with a large \\nnumber of books, which is typical for bookstores, libraries or \\nwarehouses. Search of the definite books based on the use of \\na neural network capable to localize book spines can be more \\nefficient and faster than in case if it is  performed manually. On', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='the basis of such a neural network, it is possible to create a \\nspecialized application that can act, for example, as an \\ninventory search engine or an independent mobile application.  \\nThis article discusses the development of a neural  network \\nfor detecting books on bookshelves with the use of book \\nspines. Book detection is supposed to be performed in real \\ntime. In this regard, it is necessary to build and train a neural \\nnetwork that can recognize a sufficient number of frames per', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='secon d and at the same time give fairly accurate results.  \\nThe neural network should analyze the input image for the \\npresence of books on it and provide the user with information about the detected objects. In other words, at the output the \\nlocation of the books  should be marked on the submitted \\nimage with the use of  a bounding box (\"border\" around the \\nspine of the detected book) located on the book spine.  \\nThe main tasks of this work are to create and compare', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='several models of the CNN for detecting books and thei r \\nfurther training and comparison of the results obtained in order \\nto identify the optimal model.  \\nII. YOLO  CONVOLUTIONAL NETWO RK \\nNowadays deep convolutional networks successfully act \\nas the systems of deep learning and show good results in \\ndifferent tasks, for  instance, in case of image classification, \\nobject detection and segmentation.  \\nThe typical architecture of CNN is based on the mixture of', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='convolutional and pooling layers. While passing through it \\npicture transforms into the feature map which consequently \\ngoes to fully -connected layers.  \\nYou Only Look Once ( YOLO ) architecture refers to the \\ntype of one -shot detectors  [1-3]. This type of networks is \\nnotable for remarkable speed of work, however precision of \\nobtained results due to the single passing of the ima ge through \\nthe network can be a little lower. It can be quite useful to put', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='in practice  this type of network as a basement for software if \\nit deals with the real time functioning and allows to recognize \\nsufficient quantity of frames.  \\nAccording to the resu lts of researches [3] the last \\nmodification of YOLO – YOLOv3 is turning out to \\ndemonstrate the highest speed of work and precision in \\ncomparison with other networks of the same type of detectors. \\nYOLO can be used for solving problems of different type s, for', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='example, for ship detection [ 4], vehicle traffic analysis [ 5], \\ntraffic signposts [ 6], bridge damage detection [ 7], in case of \\nmedical tasks [ 8], etc.  \\nOne of the important features of YOLO -based networks is \\nthe use of grid of a definite size which superim poses the input \\nimage and divide s it into a number of cells. Each cell is given \\nan array of predicted values: x and y – the coordinates of the \\nbounding box (upper left and lower right points) or the', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='coordinates of the center of the bounding box, w and h – width \\nand height of the bounding box and also С – the probability of \\nthe class to which object detected in the image belong. The \\ninput image of the neural network is divided into S cells. For \\nAdvances in Intelligent Systems Research, volume 174\\nProceedings of the 8th Scientific Conference on Information Technologies for\\nIntelligent Decision Making Support (ITIDS 2020)\\nCopyright © 2020 The Authors. Published by Atlantis Press B.V.', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='This is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 218', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 0}),\n",
              " Document(page_content='each object in the picture there is a cell which is responsible \\nfor its prediction (the cell where the center of the object falls \\nin). For each cell prediction is performed for B bounding \\nboxes and C probabilistic estimates of classes. Thus, the \\nterminal output takes the shape of S x S x (B х 5 +  C)-\\ndimensional tensor.  \\nAfter receiving the input image the network performs \\nnecessary operations to split it into cells. During this process', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='the non -max suppression algorithm is applied with the \\npurpose of elimination of unnecessary predictions.  \\nThe input data for the algorithm  include a list with the \\nfollowing elements: values of the specified bounding boxes, \\nthe probability of the presence of the desired object on the \\nimage and the overlap thresholds. As a result the output data \\npresent a list of predictions which have success fully passed \\nsome kind of filtering which can be described as a set of', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content=\"definite actions: the preliminary created empty list is filled \\nwith the prediction with the maximum value of the probability \\nof the object's presence in the image, then Intersection Ove r \\nUnion ( IOU) values for this and all other predictions are \\ncalculated. If IOU values exceed the value of specified overlap \\nthreshold the predicted bounding box it belongs to is deleted. \\nIn the end, the frame coordinates (or center coordinates with\", metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='width a nd height) are obtained in the last convolutional layer.  \\nOur network is intended to be used for detection of objects \\nof a single class. It means that the output data of our network \\nwill be presented in the form of a massive which contains 4 \\nvalues.  \\nAccordi ng to [ 1], the YOLO network has a number of \\nadvantages such as very high speed of work, which allows it \\nto recognize up to 35 frames per second (according to the \\nspeed of the latest version of YOLO on the COCO data set', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='was 35 fps  [9]), possibility to opera te with the whole image at \\nonce, not with its individual parts and to study generalized \\nrepresentations of objects. All these peculiarities cause better \\nwork of the network on new data and fewer mistakes while \\nseparating the desired objects from the backgr ound.  \\nBatch -normalization, a method mostly oriented on \\nnormalizing input layers by scaling the dataset, can be applied \\nin order to increase the stabilization and performance of neural', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='networks. Batch -normalization is based on preliminary \\nprocessing of data  that the network will later operate on, to a \\nstate where the set of values submitted to the input of the \\nneural network which is characterized by a zero mathematical \\ndistribution and the variance is 0.  \\nLeaky ReLU which a modified version of the normal \\nReLU function is often used as an activation function in \\nYOLO networks. This modification allows successfully use', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='the activation function in cases where conventional ReLU \\nfails, for example, if the neural network training speed is too \\nhigh.  \\nIII. NETWORK FOR BOOK D ETECTION  \\nYOLO -based network for book detection was trained from \\nscratch, without using ready -made scales. A specially created \\ndataset based on 500 images was used in process of network \\ntraining.  \\nEach image corresponds to one of the markup files in xml', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='form at with the total information for the objects of the desired class: class labels, coordinates of the bounding box for each \\nobject, name of the file, height and width of the image. The \\ncoordinates of the bounding box are xmin and ymin for the \\nupper -left cor ners and xmax and ymax for lower -right ones. \\nImage markup was performed in a form similar to the markup \\nform of the PascalVOC dataset.  \\nIn our case the used data set is not large enough and it is', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='sufficiently homogeneous so we decide to apply the \\naugmentati on technique to expand the initial data volume \\ncontained in the dataset we have already created. This means \\nthat new elements were artificially generated on the base of \\nthe original dataset. Augmentation was performed on \\nrandomly selected images from the d ataset and included the \\nactions such as scaling, rotation, flip, cropping distortion of \\ncolours and so on.  \\nTo assess the quality of neural networks various metrics', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='are used. Among them there is one of the most popular metrics \\nknown as average precision (AP ). It is used in various tasks, \\nfor example, in cases when we need to evaluate the quality of \\nranking, classification, or objects detection. The AP for a \\nseries of queries can be defined as the average value of the \\naverage accuracy estimates for each query  [10]. In our case, \\nthe metric determines the percentage of correctly detected \\nobjects. Mathematically, AP is a calculation of the values of', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='p(r) – the recall function -in the interval of values r = (0; 1).  \\nFor the programming language basement for our netw orks \\nwe use Python 3.6. Creation of CNN structure for book \\ndetection was done with the use of Python libraries for deep \\nlearning – Keras 2.2.4 and TensorFlow 1.12.0.  \\nThe dataset on which the deep neural network was trained \\nconsist s of 500 images with 5,245  book spines  depicted on \\nthem. Further, the dataset was divided in certain percentage :', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='into training (60% of pictures), validation (10%) and test \\n(30%) sets. Thus, 300 pictures with frames were included in \\nthe test set, 50 in the validation set, and 150 in  the test set. \\nMoreover, the number of frames in the markup for each \\npicture was individual.  \\nIn the process of creating a neural network, several \\nvariants of the network structure were tested, including one \\nclose to Tiny YOLOv3 . Tiny YOLOv3  is a modificati on of', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='YOLO architecture which is characterized by 2 outputs in \\ncomparison with standard 3 outputs in full YOLO networks \\nversion. It also has a simplified layer structure which ma kes \\nTiny YOLOv3  networks more adaptable for functioning on a \\nmobile platform.  \\nFor Y OLO v3 architecture networks there are 3 outputs, \\neach is responsible for recognizing objects of a certain size; so \\none of the outputs with the highest resolution is responsible', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='for detecting small objects, the output with the lowest \\nresolution detect s objects that are larger. Accordingly, an \\noutput with an average resolution value recognizes medium -\\nsized objects.  \\nIV. RESULTS  \\nIn addition to the standard Y OLO v3 with 3 outputs, we \\nalso tested the modified Y OLO v3 network with 2 outputs, \\nwhich are typical for Tiny YOLOv3 , however, in this case \\nAdvances in Intelligent Systems Research, volume 174\\n219', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 1}),\n",
              " Document(page_content='there is high probability of overfitting, while the standard \\nnetwork can be trained further in order to improve the results.  \\nAll of the deep neural networks models were  trained \\nduring 800 epochs . We trained our network models for book \\ndetection on the GPU Nvidia GeForce GTX 1060 6GB  with \\nfollowing set of network parameters:  \\n  \\nFig. 1. Changes in error values in training and validation datasets for \\nYOLOv3 Mo del', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 2}),\n",
              " Document(page_content='YOLOv3 Mo del \\n \\n Fig. 2. Changes in error values in training and validation datasets for \\nYOLOv3 Model with 2 outputs  \\n \\n Fig. 3. Changes in error values in training and validation datasets for \\nTiny-YOLOv3 Model  \\n \\nFig. 4. Example of the neural network work in book detection for \\nYOLOv3 Model  \\n \\nFig. 5. Example of the neural network work in book detection for \\nYOLOv3 Model with 2 outputs  \\n \\nFig. 6. Example of the neural network work in book detection for Tiny -\\nYOLOv3 Model', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 2}),\n",
              " Document(page_content='YOLOv3 Model  \\nAdvances in Intelligent Systems Research, volume 174\\n220', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 2}),\n",
              " Document(page_content='1. size of input image – 448x448x3; \\n2. optimizer – Adam;  \\n3. the learning rate – 0,0001;  \\n4. batch -size – 2 samples;  \\n5. metric for assessing the correctness of the class  \\ndefinition – AP. \\nDuring training of the deep neural networks the error \\nfunction was minimized. The error function can be defined as \\nthe sum of errors from each output, so in the models with two \\noutputs the error is less than in the models with three once.', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='Thus, it is neces sary to evaluate the network efficiency by the \\nAP value.  \\nThe training graphs with the obtained error values for 3 \\ndifferent network models – standard YOLOv3  with 3 outputs , \\nYOLOv3 with 2 outputs and Tiny -YOLOv3 are shown in Fig. \\n1, 2, 3.  \\nTable 1 shows the best values of AP and loss values for 3 \\ndatasets – training, validation and testing obtained during \\nnetwork training for each of 3 tested network models.  \\nTABLE I.  THE  RESULTS  OF YOLO -BASED  NETWORKS', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='COMPARISON  \\nYOLO \\nmodel  Training set  Validation set  Testing set  \\nAP Loss AP Loss AP Loss \\nYOLOv3 \\n(3 outputs)  93.74%  7.38 91.76%  6.57 94.43%  6.74 \\nYOLOv3 \\n(2 outputs)  89.29 % 4.53 91.07 % 3.72 92.35 % 4.42 \\nTiny \\nYOLOv3  84.98 % 5.96 88.42 % 6.23 88.75 % 6.98 \\n \\nFig. 4, 5 and 6 show the examples of the functioning of the \\nneural networks after training. Above each frame (bounding \\nbox with required object) there are the probability values', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='which corresponds to the probability of found object to belong \\nto a given class. We see that the neural network s quite \\nconfidently learne d to recognize and detect books on book \\nroots.  However, the best results were achieved when using the \\noriginal YOLOv3 architecture structure.  \\nIn our case, experiments with reducing of the number of \\nlayers did not bring much success, while increasing their', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='number on the contrary led to improved results. However, if we increase the number of layers, the network may become \\nheavier and the speed of its operation may decrease. We plan \\nto discuss this issue in the next papers.  \\nV. CONCLUSIONS  \\nThis article is dedicate d to the creation of a neural network \\nfor book detection on book spines . As a basis for it several \\nmodels of YOLOv3 architecture which is distinguished by a \\nsignificant speed of obtaining output data and a relatively', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='small error value were used. In the process of training neural \\nnetworks a comparison of the quality of their work was made \\nand the best network was selected. The results obtained after \\ntraining a deep neural network showed that the network has \\nlearned how to recognize and detect objects of a  given class \\nconfidently. In the future we are planning to continue our work \\non its improvement.  \\nREFERENCES  \\n[1] J. Redmon, S. Divvala, R. Girshick, A. Farhadi. “You only look once:', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='Unified, real -time object detection,” Proc. 2016 IEEE Conference on \\nComputer Vi sion and Pattern Recognition, pp. 779 –788, 2016.  \\n[2] J. Redmon, A. Farhady. “Yolo9000: Better, faster, stronger,” Proc. \\n2017 IEEE Conference on Computer Vision and Pattern Recognition, \\npp. 6517 –6525, 2017.  \\n[3] J. Redmon, A. Farhady. “YOLOv3: An Incremental Improvement,” \\nProc. 2018 IEEE Conference on Computer Vision and Pattern \\nRecognition, 2018.', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='[4] Y. Chang , A. Anagaw, L. Chang, Y. Chun Wang, C. Hsiao, W. Lee. \\n“Ship Detection Based on YOLOv2 for SAR Imagery,” Rem ote \\nSensing – Open Access Journal, 2 April 2019.  \\n[5] C. Rajesh Babu, G. Anirudh. “Vehicle Traffic Analysis Using Yolo,” \\nEurasian Journal of Analytical Chemistry, vol. 13, pp. 345 –350, 2018.  \\n[6] A. V. Devyatkin, D. M. Filatov. “Neural network traffic signs detectio n \\nsystem development,”  Proc. International conference on soft', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content='computing and measurement, vol. 1, pp. 189 -192, 2019 [Nejrosetevaya \\nsistema obnaruzheniya znakov dorozhnogo dvizheniya].  \\n[7] C. Zhang, C.C. Chang, M. Jamshidi. “Bridge Damage Detection using \\na Sing le-Stage Detector and Field Inspection Images,” Computer \\nSociety, 8 April 2018.  \\n[8] K.S. Kurochka, T.V. Luchsheva, K.A. Panarin. “Localization of human \\npercentages on X -ray images with use of Darknet YOLO,” Doklady \\nBGUIR, Vol. 113, Nо. 3, pp. 32 –38, 2018 [Loka lizaciya pozvonkov', metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3}),\n",
              " Document(page_content=\"cheloveka na rentgenovskih izobrazheniyah s ispol'zovaniem \\nDARKNET YOLO].  \\n[9]  “YOLO: Real -Time Object Detection ”.  Source: \\nhttps://pjreddie.com/darknet/yolo/  \\n[10] T.C. Arlen. “Understanding the mAP Evaluation Metric for Object \\nDetection,” Medium, 1 March 2018.  \\n \\n \\n  \\n \\nAdvances in Intelligent Systems Research, volume 174\\n221\", metadata={'source': 'D:\\\\cb\\\\Research_of_YOLO_Architecture_Models_in_Book_Detec.pdf', 'page': 3})]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM6GNG-6ud8F",
        "outputId": "cb15073d-6526-41e0-b967-35b849587303"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxGg6pSiasou",
        "outputId": "73017992-f050-47b1-f39e-c8d2241816fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. size of input image – 448x448x3; \n",
            "2. optimizer – Adam;  \n",
            "3. the learning rate – 0,0001;  \n",
            "4. batch -size – 2 samples;  \n",
            "5. metric for assessing the correctness of the class  \n",
            "definition – AP. \n",
            "During training of the deep neural networks the error \n",
            "function was minimized. The error function can be defined as \n",
            "the sum of errors from each output, so in the models with two \n",
            "outputs the error is less than in the models with three once.\n"
          ]
        }
      ],
      "source": [
        "print(text_chunks[30].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "8LLAIWXK1WKF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psu_Mqb11JLx",
        "outputId": "1bb5ceea-2e48-4f72-8e06-d0f5e5760c72"
      },
      "outputs": [],
      "source": [
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Get Google API key from environment variable or set it if not present\n",
        "\n",
        "if not api_key:\n",
        "    api_key = getpass(\"Provide your Google API key here: \")\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "# Print to verify the API key (for debugging purposes only, remove in production)\n",
        "# print(f\"Google API key set: {os.environ['GOOGLE_API_KEY']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfPPC2vqayNP",
        "outputId": "8a8184b2-0786-40a2-aa28-33e15e924942"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# result = genai.embed_content(\n",
        "#     model=\"models/embedding-001\",\n",
        "#     content=\"What is the meaning of death and life?\",\n",
        "#     task_type=\"RETRIEVAL_QUERY\")\n",
        "\n",
        "# # 1 input > 1 vector output\n",
        "# print((result['embedding']))\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "len(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "L-My2CFucGBp"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PINECONE_API_KEY = os.environ.get('pinecone_api_key')\n",
        "\n",
        "# print(PINECONE_API_KEY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "RZEwBfwoiXi2"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "index = pinecone.Index(index, host=\"https://chatbot-658rjfl.svc.aped-4627-b74a.pinecone.io\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MQFOO7wFN37",
        "outputId": "8837cb01-6674-4aa2-ac3e-2f4ec3862a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_pinecone in .\\env\\lib\\site-packages (0.1.1)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in .\\env\\lib\\site-packages (from langchain_pinecone) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in .\\env\\lib\\site-packages (from langchain_pinecone) (1.26.4)\n",
            "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in .\\env\\lib\\site-packages (from langchain_pinecone) (3.2.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.1.65)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.7.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in .\\env\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (8.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in .\\env\\lib\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in .\\env\\lib\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in .\\env\\lib\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.12.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in .\\env\\lib\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in .\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in .\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in .\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.18.3)\n",
            "Requirement already satisfied: colorama in .\\env\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\env\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\env\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5hP4ntXD0n2",
        "outputId": "1da921c9-49b7-40e4-fbc6-1cca4757073b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x2d98129db90>"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "os.environ[\n",
        "  'PINECONE_API_KEY'] = '04f52ace-6a5b-4932-be1a-4a9e1f5bdc81'\n",
        "\n",
        "index_name = \"chatbot\"\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "docsearch = PineconeVectorStore.from_texts(\n",
        "   [t.page_content for t in text_chunks],\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")\n",
        "docsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "UR09a9Kt-nq6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['PineconeVectorStore', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000002D98129DB90>)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch.as_retriever()\n",
        "# pip install --upgrade langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "m3xXpZ0E_IyY"
      },
      "outputs": [],
      "source": [
        "query= \"YOLOv7 outperforms which models?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "oB6NRofnZgKN",
        "outputId": "f38cf2cb-8c88-4f2c-dd74-387aab5d83e1"
      },
      "outputs": [
        {
          "ename": "PineconeApiValueError",
          "evalue": "Unable to prepare type Repeated for serialization",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPineconeApiValueError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[126], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the similarity search\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdocsearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:247\u001b[0m, in \u001b[0;36mPineconeVectorStore.similarity_search\u001b[1;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    230\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:192\u001b[0m, in \u001b[0;36mPineconeVectorStore.similarity_search_with_score\u001b[1;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m     namespace: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_by_vector_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:209\u001b[0m, in \u001b[0;36mPineconeVectorStore.similarity_search_by_vector_with_score\u001b[1;34m(self, embedding, k, filter, namespace)\u001b[0m\n\u001b[0;32m    207\u001b[0m     namespace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namespace\n\u001b[0;32m    208\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 209\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    217\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\utils\\error_handling.py:10\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\data\\index.py:399\u001b[0m, in \u001b[0;36mIndex.query\u001b[1;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m sparse_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_sparse_values_arg(sparse_vector)\n\u001b[0;32m    386\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args(\n\u001b[0;32m    387\u001b[0m     [\n\u001b[0;32m    388\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m, vector),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     ]\n\u001b[0;32m    398\u001b[0m )\n\u001b[1;32m--> 399\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQueryRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_OPENAPI_ENDPOINT_PARAMS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_OPENAPI_ENDPOINT_PARAMS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_query_response(response)\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:772\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m \n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api\\data_plane_api.py:844\u001b[0m, in \u001b[0;36mDataPlaneApi.__init__.<locals>.__query\u001b[1;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    842\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_request\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    843\u001b[0m     query_request\n\u001b[1;32m--> 844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:834\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    830\u001b[0m     header_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mselect_header_content_type(\n\u001b[0;32m    831\u001b[0m         content_type_headers_list)\n\u001b[0;32m    832\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m header_list\n\u001b[1;32m--> 834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendpoint_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp_method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_check_return_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcollection_format\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:409\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    418\u001b[0m                                                method, path_params,\n\u001b[0;32m    419\u001b[0m                                                query_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m                                                _request_timeout,\n\u001b[0;32m    428\u001b[0m                                                _host, _check_type))\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# body\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m body:\n\u001b[1;32m--> 181\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_for_serialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# auth setting\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params_for_auth(header_params, query_params,\n\u001b[0;32m    185\u001b[0m                             auth_settings, resource_path, method, body)\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:273\u001b[0m, in \u001b[0;36mApiClient.sanitize_for_serialization\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares data for transmission before it is sent with the rest client\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mIf obj is None, return None.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03mIf obj is str, int, long, float, bool, return directly.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m:return: The serialized form of data.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ModelNormal, ModelComposed)):\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_for_serialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, io\u001b[38;5;241m.\u001b[39mIOBase):\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_file_data_and_close_file(obj)\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:274\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares data for transmission before it is sent with the rest client\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mIf obj is None, return None.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03mIf obj is str, int, long, float, bool, return directly.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m:return: The serialized form of data.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ModelNormal, ModelComposed)):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m--> 274\u001b[0m         key: \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_for_serialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m model_to_dict(obj, serialize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    275\u001b[0m     }\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, io\u001b[38;5;241m.\u001b[39mIOBase):\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_file_data_and_close_file(obj)\n",
            "File \u001b[1;32md:\\cb\\env\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:288\u001b[0m, in \u001b[0;36mApiClient.sanitize_for_serialization\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msanitize_for_serialization(val) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiValueError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to prepare type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for serialization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
            "\u001b[1;31mPineconeApiValueError\u001b[0m: Unable to prepare type Repeated for serialization"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Perform the similarity search\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G89HDKlh__JQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
